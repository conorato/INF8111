{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Market Basket Analysis \n",
    "INF8111 - Fouille de données, Fall 2019\n",
    "### Team Components\n",
    "    - William Harvey (1851388)\n",
    "    - Jérémie Miglierina ()\n",
    "    - Claudia Onorato (1845448)\n",
    "\n",
    "**Authors**: Rodrigo Randel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date et directives de remise\n",
    "Vous remettrez ce fichier nommé TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb dans la boîte de remise sur moodle. \n",
    "\n",
    "Tout devra être remis avant le **3 decembre à 23h55**.\n",
    "\n",
    "## Market Basket Analysis\n",
    "\n",
    "Market Basket Analysis (MBA) is a data mining analytics technique to uncover associations between products or product grouping. By exploring interesting patterns from an extensive collection of data, MBA aims to understand/reveal customer purchase behaviors based upon the theory that if you purchased a certain set of products, then you are more (or less) likely to buy another group of products. In other words, MBA allows retailers to identify the relationship between the items that customers buy, revealing patterns of items often purchased together.\n",
    "\n",
    "A widely approach to explore these patterns is by constructing ***association rules*** such as:\n",
    "- **if** bought *ITEM_1* **then** will buy *ITEM_2* with **confidence** *X*.\n",
    "\n",
    "These associations do not have to be 1-to-1 rules. They can involve many items. For example, a person in a supermarket may add eggs to his/her cart, then an MBA application may suggest that the person will also buy some bread and/or flour: \n",
    "    \n",
    "+ **if** bought *EGGS* **then** will buy [*BREAD* with confidence *0.2*; *FLOUR* with confidence 0.05].\n",
    "\n",
    "However, if the person now decides to add flour to his/her cart, the new association rule could be as showing below, suggesting ingredients to make a cake.\n",
    "\n",
    "+ **if** bought [*EGGS, FLOUR*] **then** will buy [*SUGGAR* with confidence 0.45; BAKING POWDER with confidence 0.12; *BREAD* with confidence *0.03*].\n",
    "\n",
    "\n",
    "There are many real scenarios where MBA plays a central role in data analysis, such as supermarket transactions, online orders or credit card history. Marketers may use these association rules to allocate correlated products close to each other on store shelves or make online suggestions so that customers buy more items. Some questions that an MBA can usually help retailers to answer are:\n",
    "\n",
    "    - What items are often purchased together?\n",
    "    - Given a basket, what items should be suggested?\n",
    "    - How should items be placed together on the shelves?\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your goal in this TP is to develop an MBA algorithm for revealing patterns by creating association rules in a big dataset with more than three millions supermarket transactions. However, mining association rules for large datasets is a very computationally intensive problem, which makes it almost impractical to perform it without a distributed system. Hence, to run your algorithm, you will have access to a distributed cloud computing cluster with hundreds of cores. \n",
    "\n",
    "To this end, a **MapReduce** algorithm will be implemented upon the [Apache Spark](http://spark.apache.org) framework, a fast cluster computing system. In a nutshell, Spark is an open source framework designed with a *scale-out* methodology which makes it a very powerful tool for programmers or application developers to perform a massive volume of computations and data processing in distributed environments. Sparks provides high-level APIs that make it easy to build parallel apps without needing to worry about how your code and data are parallelized/distributed thought the computing cluster. Spark does it all for you.\n",
    " \n",
    "The implementation will follow the Market Basket Analysis algorithm presented by Jongwook Woo and Yuhang Xu (2012). The image **workflow.svg** Illustrates the algorithm's workflow, and is to be used for consultation throughout this TP. The blue boxes are the ones where you must implement a method to perform a map or reduce function, and the grey boxes represent their expected output. **All these operations are explained in details in the following sections.** \n",
    "\n",
    "<!---\n",
    "# ![scale=0.5](workflow.svg \"Algorithm Workflow\")\n",
    "-->\n",
    "\n",
    "## 1. Setting up Spark\n",
    "\n",
    "Spark runs on both Windows and UNIX-like systems (e.g., Linux, Mac OS). It's easy to run locally on one machine — all you need is to have Java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. It is recommended that you have the **JDK v8** installed in your system. If you haven't, go to [Java's web page](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) to download and install a Java Virtual Machine. Remember to set the environment variable JAVA_HOME if your installation does not do it automatically for you. \n",
    "\n",
    "The interface between Python and Spark is done through **PySpark**, which can be obtained following the sequence below:\n",
    "\n",
    "1. First, go to http://spark.apache.org/downloads \n",
    "2. Select the newest Spark release and the Pre-built for Apache Hadoop 2.7 package \n",
    "3. Click for download **spark-2.4.4-bin-hadoop2.7.tgz** and unzip it in any folder of your preference. \n",
    "4. Next, export the following variables to link PYSPARK (Spark's python interface) to your python distribution in yout `~/.bash_profile` file.\n",
    "```\n",
    "    - export SPARK_HOME=/path/to/spark-2.4.4-bin-hadoop2.7\n",
    "    - export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n",
    "    - export PYSPARK_PYTHON=/path/to/your/python3\n",
    "```\n",
    "5. Run `source ~./bash_profile` to effictate the changes and reinitializate this jupyter notebook session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Product Counting Example \n",
    "\n",
    "To test your installation and start to get familiarized with Spark, we will follow an example that counts how many times the products of a toy dataset were purchased.\n",
    "\n",
    "The main entry point to start programming with Spark is the [RDD API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), an excellent Spark abstraction to work with the MapReduce framework.  RDD is a collection of elements partitioned across the nodes of the cluster that can operate in parallel. In other words, RDD is how Spark keeps your data ready to perform some function (e.g., a map or reduce function) in parallel. **Do not worry if this still sounds confusing, it will be clear once you start implementing**. However, it is part of this TP to study/consult the [Spark python API](https://spark.apache.org/docs/latest/api/python/) and learn how to use it. Some useful functions that the RDD API offers are:\n",
    "\n",
    "1. **map**: return a new RDD by applying a function to each element of this RDD.\n",
    "2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n",
    "3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
    "4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n",
    "5. **groupByKey**: group the values for each key in the RDD into a single sequence\n",
    "6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n",
    "7. **sample**: return a sampled subset of this RDD\n",
    "8. **count**: return the number of elements in this RDD.\n",
    "9. **filter**: return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset\n",
      "+--------+-----------+\n",
      "|order_id|transaction|\n",
      "+--------+-----------+\n",
      "|       1|      a;b;c|\n",
      "|       2|      a;b;d|\n",
      "|       3|        b;c|\n",
      "|       4|        b;c|\n",
      "+--------+-----------+\n",
      "\n",
      "Toy dataframe as a RDD object (list of Row objects):\n",
      "\t [Row(order_id='1', transaction='a;b;c'), Row(order_id='2', transaction='a;b;d'), Row(order_id='3', transaction='b;c'), Row(order_id='4', transaction='b;c')]\n",
      "\n",
      "Mapped products:\n",
      "\t [('a', 1), ('b', 1), ('c', 1), ('a', 1), ('b', 1), ('d', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]\n",
      "\n",
      "Reduced (merged) products:\n",
      "\t [('a', 2), ('b', 4), ('c', 3), ('d', 1)]\n",
      "\n",
      "Visualizing as a dataframe:\n",
      "+-------+-------------+\n",
      "|product|count_product|\n",
      "+-------+-------------+\n",
      "|      a|            2|\n",
      "|      b|            4|\n",
      "|      c|            3|\n",
      "|      d|            1|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def map_to_product(row):\n",
    "    \"\"\"\n",
    "    Map each transaction into a set of KEY-VALUE elements.\n",
    "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
    "    \"\"\"\n",
    "    products = row.transaction.split(';') # split products from the column transaction\n",
    "    for p in products:\n",
    "        yield (p, 1)\n",
    "\n",
    "def reduce_product_by_key(value1, value2):\n",
    "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
    "    return value1+value2\n",
    "\n",
    "# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "# Read a toy dataset\n",
    "toy = spark.read.csv('toy.csv', header=True)\n",
    "print(\"Toy dataset\")\n",
    "toy.show()\n",
    "\n",
    "# Obtain a RDD object to call a map function\n",
    "toy_rdd = toy.rdd\n",
    "print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "# Map function to identify all products\n",
    "toy_rdd = toy_rdd.flatMap(map_to_product)\n",
    "print(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "# Reduce function to merge values of elements that share the same KEY\n",
    "toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n",
    "print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "print(\"\\nVisualizing as a dataframe:\")\n",
    "toy_rdd.toDF([\"product\", \"count_product\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Working the Spark's Dataframe\n",
    "\n",
    "In the example above, we briefly used a Spark's Dataframe class, but only to obtain an RDD object with ```toy.rdd``` and to print the data as a structured table with the ```show()``` function. However, [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#) is a big part of the current Spark release and is built upon the RDD API. It is a distributed collection of rows under named columns, the same as a table in a relational database. Spark's Dataframe works similarily as [Pandas'](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). In fact, we can export (obtain) a Spark's data frame to (from) a pandas' data frame with the function ```toPandas()``` (```spark.createDataFrame```).\n",
    "\n",
    "A central functionality of the data frame is to profit from the [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), a module that allows SQL queries over structured data. For example, the same 'product counting example' could have been implemented as a sequence of SQL operations over the data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'products': exploding the transaction's products to a new row\n",
      "+--------+-----------+--------+\n",
      "|order_id|transaction|products|\n",
      "+--------+-----------+--------+\n",
      "|       1|      a;b;c|       a|\n",
      "|       1|      a;b;c|       b|\n",
      "|       1|      a;b;c|       c|\n",
      "|       2|      a;b;d|       a|\n",
      "|       2|      a;b;d|       b|\n",
      "|       2|      a;b;d|       d|\n",
      "|       3|        b;c|       b|\n",
      "|       3|        b;c|       c|\n",
      "|       4|        b;c|       b|\n",
      "|       4|        b;c|       c|\n",
      "+--------+-----------+--------+\n",
      "\n",
      "Couting unique products:\n",
      "+--------+-------------+\n",
      "|products|count_product|\n",
      "+--------+-------------+\n",
      "|       b|            4|\n",
      "|       c|            3|\n",
      "|       a|            2|\n",
      "|       d|            1|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Creates a new column, products, with all products appering in each transaction\n",
    "print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n",
    "df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n",
    "df_toy.show()\n",
    "\n",
    "# Performs a select query and group rows by the product name, aggreagating by counting\n",
    "print('Couting unique products:')\n",
    "df_toy.select(df_toy.products)\\\n",
    "      .groupBy(df_toy.products)\\\n",
    "      .agg(f.count('products').alias('count_product'))\\\n",
    "      .sort('count_product', ascending=False)\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the same SQL operations performed above could have been done with a traditional SQL language query as showing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|products|product_count|\n",
      "+--------+-------------+\n",
      "|       b|            4|\n",
      "|       c|            3|\n",
      "|       a|            2|\n",
      "|       d|            1|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a relational table TOY in the Spark session\n",
    "df_toy.createOrReplaceTempView(\"TOY\")\n",
    "\n",
    "spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n",
    "          \" FROM TOY t\"\n",
    "          \" GROUP BY t.products\"\n",
    "          \" ORDER BY product_count DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These SQL concepts are being mentioned here because they will be useful to us during the TP, mainly in Section 3, to manipulate the supermarket data, which is structured in data frames. Then, if you are not familiar with SQL, it is recommended that you follow a [tutorial](https://www.w3schools.com/sql/) to understand the basics.\n",
    "\n",
    "## 2. MBA Algorithm \n",
    " The following sections explain how you should develop each step of the MapReduce algorithm for our supermarket application. Figure workflow.png illustrates each step of the algorithm.\n",
    " \n",
    "### 2.1 Map to Patterns (10 points)\n",
    "For a given a set of transactions (i.e., the rows of our toy dataset), each transaction must be **mapped** into a set of *purchase patterns* found within the transaction. Formally, these patterns are subsets of products that represent a group of items bought together. \n",
    "    \n",
    "For the MapReduce framework, each pattern must be created as a *KEY-VALUE* element, where they KEY can take the form of a singleton, a pair or a trio of products that are present in the transaction. More precisely, for each transaction, the mapping function must generate all possible **unique** subsets of size **ONE, TWO or THREE**.  The VALUE associated with each KEY is the number of times that the KEY appeared in the transaction (if we assume that no product appears more than once in the transaction, this value is always equal to one). \n",
    "\n",
    "Now, implement the  **map_to_patterns** function that receives a transaction (a row from the data frame) and returns the patterns found in the transaction. The mapped elements are a tuple (KEY, VALUE), where KEY is also a tuple of product names. It is crucial to notice that, since each entry (transaction) of the map function will **yield** more than one KEY-VALUE element, a *flatMap* must be invoked for this step.\n",
    "\n",
    "For the toy dataset, the expected output is similar to:\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 10pt; line-height: 1.1; height: auto; width: 18em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+-----------+\n",
    "|       patterns|occurrences|\n",
    "+---------------+-----------+\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'c')|          1|\n",
    "|     ('a', 'c')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'd')|          1|\n",
    "|     ('a', 'd')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'd')|          1|\n",
    "|         ('d',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "+---------------+-----------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|       patterns|occurrences|\n",
      "+---------------+-----------+\n",
      "|         ('a',)|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|     ('a', 'b')|          1|\n",
      "|     ('a', 'c')|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "|('a', 'b', 'c')|          1|\n",
      "|         ('a',)|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('d',)|          1|\n",
      "|     ('a', 'b')|          1|\n",
      "|     ('a', 'd')|          1|\n",
      "|     ('b', 'd')|          1|\n",
      "|('a', 'b', 'd')|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "BASKET_SIZES = 3\n",
    "\n",
    "def format_tuples(pattern):\n",
    "    \"\"\"\n",
    "    Used for visualizition.\n",
    "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
    "    (a,b,c) -> '(a,b,c)'\n",
    "    \"\"\"\n",
    "    return (str(pattern[0]), str(pattern[1]))\n",
    "\n",
    "def map_to_patterns(row):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    The key is a combination of products in the current transaction and value is always 1.\n",
    "    \"\"\"\n",
    "    products = row.transaction.split(';')\n",
    "    \n",
    "    for basket_size in range(BASKET_SIZES):\n",
    "        for combination in combinations(products, basket_size + 1):\n",
    "            yield (combination,1)\n",
    "    \n",
    "\n",
    "toy_rdd = toy.rdd\n",
    "patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n",
    "\n",
    "# Output as dataframe\n",
    "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reduce patterns (2.5 points)\n",
    "Once different CPUs processed the transactions, a **reduce** function must take place to combine identical KEYS (the subset of products) and compute the total number of its occurrences in the entire dataset. In other words, this reduce procedure must sum the *VALUE* of each identical KEY.\n",
    "\n",
    "Create a **reduce_patterns** function below that must sum the VALUE of each pattern.\n",
    "For the toy dataset, the expected output is:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 10pt; line-height: 1.1; height: auto; width: 24em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+--------------------+\n",
    "|       patterns|combined_occurrences|\n",
    "+---------------+--------------------+\n",
    "|         ('a',)|                   2|\n",
    "|     ('a', 'b')|                   2|\n",
    "|('a', 'b', 'c')|                   1|\n",
    "|     ('a', 'c')|                   1|\n",
    "|         ('b',)|                   4|\n",
    "|     ('b', 'c')|                   3|\n",
    "|         ('c',)|                   3|\n",
    "|('a', 'b', 'd')|                   1|\n",
    "|     ('a', 'd')|                   1|\n",
    "|     ('b', 'd')|                   1|\n",
    "|         ('d',)|                   1|\n",
    "+---------------+--------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|       patterns|combined_occurrences|\n",
      "+---------------+--------------------+\n",
      "|         ('a',)|                   2|\n",
      "|         ('b',)|                   4|\n",
      "|         ('c',)|                   3|\n",
      "|     ('a', 'b')|                   2|\n",
      "|     ('a', 'c')|                   1|\n",
      "|     ('b', 'c')|                   3|\n",
      "|('a', 'b', 'c')|                   1|\n",
      "|         ('d',)|                   1|\n",
      "|     ('a', 'd')|                   1|\n",
      "|     ('b', 'd')|                   1|\n",
      "|('a', 'b', 'd')|                   1|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "\n",
    "We use the same function defined in the tutorial, where we sum the values of two tuples that have the same key.\n",
    "\"\"\"\n",
    "combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_product_by_key)\n",
    "\n",
    "# Output as dataframe\n",
    "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Map to subpatterns (15 points)\n",
    "Next, another **map** function should be applied to generate subpatterns. Once again, the subpatterns are KEY-VALUE elements, where the KEY is a subset of products as well. However, creating the subpattern's KEY is a different procedure. This time, the idea is to break down the list of products of each pattern (pattern KEY), remove one product at a time, and yield the resulting list as the new subpattern KEY. \n",
    "\n",
    "For example, for a given pattern $P$ with three products, $p_1, p_2 $ and $p_3$, three new subpatterns KEYs are going to be created: (i) remove $p_1$ and yield ($p_2, p_3$); (ii) remove $p_2$ and yield ($p_1,p_3$); and (iii) remove $p_3$ and yield ($p_1,p_2$). \n",
    "\n",
    "Additionally, the subpattern's VALUE structure will also be different. Instead of just single integer value as we had in the patterns, this time a *tuple* should be created for the subpattern VALUE. This tuple contains the product that was removed when yielding the KEY and the number of times the pattern appeared. For the example above, the values should be ($p_1,v$), ($p_2,v$) and ($p_3,v$), respectively, where $v$ is the VALUE of the pattern. \n",
    "\n",
    "The idea behind subpatterns is to create **rules** such as: when the products of KEY were bought, the item present in the VALUE was also bought *v* times. Furthermore, each pattern should also yield a subpattern where the KEY is the same list of products of the pattern, but the VALUE is a tuple with a null product (None) and the number of times the pattern appeared. This element will be useful to keep track of how many times such a pattern was found and later will be used to compute the confidence value when generating the association rules. \n",
    "\n",
    "Now, implement the  **map_to_subpatterns** function that receives a pattern and yields all found subpatterns. Once again, each entry (pattern) will generate more than one KEY-VALUE element, then a flatMap function must be called.\n",
    "\n",
    "For the toy dataset, the expected output is:\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 10pt; line-height: 1.1; height: auto; width: 17em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+---------+\n",
    "|    subpatterns|    rules|\n",
    "+---------------+---------+\n",
    "|         ('a',)|(None, 2)|\n",
    "|     ('a', 'b')|(None, 2)|\n",
    "|         ('b',)| ('a', 2)|\n",
    "|         ('a',)| ('b', 2)|\n",
    "|('a', 'b', 'c')|(None, 1)|\n",
    "|     ('b', 'c')| ('a', 1)|\n",
    "|     ('a', 'c')| ('b', 1)|\n",
    "|     ('a', 'b')| ('c', 1)|\n",
    "|     ('a', 'c')|(None, 1)|\n",
    "|         ('c',)| ('a', 1)|\n",
    "|         ('a',)| ('c', 1)|\n",
    "|         ('b',)|(None, 4)|\n",
    "|     ('b', 'c')|(None, 3)|\n",
    "|         ('c',)| ('b', 3)|\n",
    "|         ('b',)| ('c', 3)|\n",
    "|         ('c',)|(None, 3)|\n",
    "|('a', 'b', 'd')|(None, 1)|\n",
    "|     ('b', 'd')| ('a', 1)|\n",
    "|     ('a', 'd')| ('b', 1)|\n",
    "|     ('a', 'b')| ('d', 1)|\n",
    "|     ('a', 'd')|(None, 1)|\n",
    "|         ('d',)| ('a', 1)|\n",
    "|         ('a',)| ('d', 1)|\n",
    "|     ('b', 'd')|(None, 1)|\n",
    "|         ('d',)| ('b', 1)|\n",
    "|         ('b',)| ('d', 1)|\n",
    "|         ('d',)|(None, 1)|\n",
    "+---------------+---------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|    subpatterns|    rules|\n",
      "+---------------+---------+\n",
      "|         ('a',)|(None, 2)|\n",
      "|         ('b',)|(None, 4)|\n",
      "|         ('c',)|(None, 3)|\n",
      "|     ('a', 'b')|(None, 2)|\n",
      "|         ('b',)| ('a', 2)|\n",
      "|         ('a',)| ('b', 2)|\n",
      "|     ('a', 'c')|(None, 1)|\n",
      "|         ('c',)| ('a', 1)|\n",
      "|         ('a',)| ('c', 1)|\n",
      "|     ('b', 'c')|(None, 3)|\n",
      "|         ('c',)| ('b', 3)|\n",
      "|         ('b',)| ('c', 3)|\n",
      "|('a', 'b', 'c')|(None, 1)|\n",
      "|     ('b', 'c')| ('a', 1)|\n",
      "|     ('a', 'c')| ('b', 1)|\n",
      "|     ('a', 'b')| ('c', 1)|\n",
      "|         ('d',)|(None, 1)|\n",
      "|     ('a', 'd')|(None, 1)|\n",
      "|         ('d',)| ('a', 1)|\n",
      "|         ('a',)| ('d', 1)|\n",
      "|     ('b', 'd')|(None, 1)|\n",
      "|         ('d',)| ('b', 1)|\n",
      "|         ('b',)| ('d', 1)|\n",
      "|('a', 'b', 'd')|(None, 1)|\n",
      "|     ('b', 'd')| ('a', 1)|\n",
      "|     ('a', 'd')| ('b', 1)|\n",
      "|     ('a', 'b')| ('d', 1)|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def map_to_subpatterns(pattern):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    patterns = pattern[0]\n",
    "    combined_occurences = pattern[1]\n",
    "\n",
    "    yield (patterns, (None, combined_occurences))\n",
    "    \n",
    "    if len(patterns) is 1:\n",
    "        return\n",
    "\n",
    "    for pattern in patterns:\n",
    "        yield (tuple(x for x in patterns if x != pattern), (pattern, combined_occurences))\n",
    "\n",
    "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
    "\n",
    "# Output as dataframe\n",
    "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reduce Subpatterns (2.5 points)\n",
    "Once again, a **reduce** function will be required to group all the subpatterns by their KEY. The objective of this reducing procedure is to create a list with all the **rules** that appeared by a KEY. Hence, the expected resulting of the reduce function is also a KEY-VALUE element, where the KEY is the subpattern's KEY, and the VALUE is a group containing all the VALUEs of the subpatterns that share the same KEY.\n",
    "\n",
    "For the toy dataset, the expected output is:\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 10pt; line-height: 1.1; height: auto; width: 36em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+-----------------------------------------+\n",
    "|subpatterns    |combined_rules                           |\n",
    "+---------------+-----------------------------------------+\n",
    "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('d', 1)]|\n",
    "|('a', 'b')     |[(None, 2), ('c', 1), ('d', 1)]          |\n",
    "|('b',)         |[('a', 2), (None, 4), ('c', 3), ('d', 1)]|\n",
    "|('a', 'b', 'c')|[(None, 1)]                              |\n",
    "|('b', 'c')     |[('a', 1), (None, 3)]                    |\n",
    "|('a', 'c')     |[('b', 1), (None, 1)]                    |\n",
    "|('c',)         |[('a', 1), ('b', 3), (None, 3)]          |\n",
    "|('a', 'b', 'd')|[(None, 1)]                              |\n",
    "|('b', 'd')     |[('a', 1), (None, 1)]                    |\n",
    "|('a', 'd')     |[('b', 1), (None, 1)]                    |\n",
    "|('d',)         |[('a', 1), ('b', 1), (None, 1)]          |\n",
    "+---------------+-----------------------------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "The simplest method is to use `groupByKey`. However, according to the [Databricks Sparks knowledge base](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html),\n",
    "this function is **slower than `reduceByKey`**, because the rows within a same node are not reduced before being passed to the next node. This leads to a lot more charge on the network and slowers the pipeline. It is thus recommended to use `reduceByKey` over `groupByKey`.\n",
    "\n",
    "In both cases, we had to do a map function to ensure all values were lists. In the case of `reduceByKey`, the values that had a unique key within all the dataset (i.e. ('a','b','c')) didn't go through the reducer, because they were unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------------------+\n",
      "|subpatterns    |combined_rules                           |\n",
      "+---------------+-----------------------------------------+\n",
      "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('d', 1)]|\n",
      "|('b',)         |[(None, 4), ('a', 2), ('c', 3), ('d', 1)]|\n",
      "|('c',)         |[(None, 3), ('a', 1), ('b', 3)]          |\n",
      "|('a', 'b')     |[(None, 2), ('c', 1), ('d', 1)]          |\n",
      "|('a', 'c')     |[(None, 1), ('b', 1)]                    |\n",
      "|('b', 'c')     |[(None, 3), ('a', 1)]                    |\n",
      "|('a', 'b', 'c')|[(None, 1)]                              |\n",
      "|('d',)         |[(None, 1), ('a', 1), ('b', 1)]          |\n",
      "|('a', 'd')     |[(None, 1), ('b', 1)]                    |\n",
      "|('b', 'd')     |[(None, 1), ('a', 1)]                    |\n",
      "|('a', 'b', 'd')|[(None, 1)]                              |\n",
      "+---------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple solution\n",
    "# combined_rules = subpatterns_rdd.groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "\n",
    "# Faster solution\n",
    "def reduce_tuples_to_list_by_key(value1, value2):\n",
    "    if type(value1) is not list:\n",
    "        value1 = [value1]\n",
    "    if type(value2) is not list:\n",
    "        value2 = [value2]\n",
    "    \n",
    "    return value1+value2\n",
    "\n",
    "def ensure_all_values_are_lists(keyvalue):\n",
    "    if type(keyvalue[1]) is not list:\n",
    "        return (keyvalue[0], [keyvalue[1]])\n",
    "    \n",
    "    return keyvalue\n",
    "\n",
    "combined_rules = subpatterns_rdd.reduceByKey(reduce_tuples_to_list_by_key).map(ensure_all_values_are_lists)\n",
    "\n",
    "# Output as dataframe\n",
    "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Map to Association Rules (15 points)\n",
    "Finally, the last step of the algorithm is to create the association rules to perform the market basket analysis. The goal of this map function is to calculate the **confidence** level of buying a product, knowing that there is already a set of products in the basket. Thus, the KEY of the subpattern is the set of products placed in the basket and, for each product present in the list of rules, i.e., in the VALUE, the confidence can be calculated as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\text{number of times the product was bought together with KEY }}{\\text{number of times the KEY appeared}}\n",
    "\\end{align*}\n",
    "\n",
    "For the example given in the Figure workflow, *coffee* was bought 20 times and, in 17 of them, *milk* was bought together. Then, the confidence level of buying *milk* knowing that *coffee* is in the basket is $\\frac{17}{20} = 0.85$, which means that in 85% of the times the coffee was bought, milk was purchased as well.\n",
    "\n",
    "Implement the **map_to_assoc_rules** function that calculates the confidence level for each subpattern.\n",
    "\n",
    "For the toy dataset, the expected output is:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 9pt; line-height: 1.1; height: auto; width: 35em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+---------------------------------------+\n",
    "|patterns       |association_rules                      |\n",
    "+---------------+---------------------------------------+\n",
    "|('a',)         |[('b', 1.0), ('c', 0.5), ('d', 0.5)]   |\n",
    "|('a', 'b')     |[('c', 0.5), ('d', 0.5)]               |\n",
    "|('b',)         |[('a', 0.5), ('c', 0.75), ('d', 0.25)] |\n",
    "|('a', 'b', 'c')|[]                                     |\n",
    "|('b', 'c')     |[('a', 0.3333333333333333)]            |\n",
    "|('a', 'c')     |[('b', 1.0)]                           |\n",
    "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0)]|\n",
    "|('a', 'b', 'd')|[]                                     |\n",
    "|('b', 'd')     |[('a', 1.0)]                           |\n",
    "|('a', 'd')     |[('b', 1.0)]                           |\n",
    "|('d',)         |[('a', 1.0), ('b', 1.0)]               |\n",
    "+---------------+---------------------------------------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------+\n",
      "|patterns       |association_rules                      |\n",
      "+---------------+---------------------------------------+\n",
      "|('a',)         |[('b', 1.0), ('c', 0.5), ('d', 0.5)]   |\n",
      "|('b',)         |[('a', 0.5), ('c', 0.75), ('d', 0.25)] |\n",
      "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0)]|\n",
      "|('a', 'b')     |[('c', 0.5), ('d', 0.5)]               |\n",
      "|('a', 'c')     |[('b', 1.0)]                           |\n",
      "|('b', 'c')     |[('a', 0.3333333333333333)]            |\n",
      "|('a', 'b', 'c')|[]                                     |\n",
      "|('d',)         |[('a', 1.0), ('b', 1.0)]               |\n",
      "|('a', 'd')     |[('b', 1.0)]                           |\n",
      "|('b', 'd')     |[('a', 1.0)]                           |\n",
      "|('a', 'b', 'd')|[]                                     |\n",
      "+---------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_to_assoc_rules(rule):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    pattern = rule[0]\n",
    "    rules = rule[1]\n",
    "    association_rules = []\n",
    "\n",
    "    pattern_frequency = next(x for x in rules if x[0] is None)[1]\n",
    "    \n",
    "    for current_rule in rules:\n",
    "        if current_rule[0] is not None:\n",
    "            association_rules.append((current_rule[0], current_rule[1]/pattern_frequency))\n",
    "        \n",
    "    return (pattern,association_rules)\n",
    "\n",
    "assoc_rules = combined_rules.map(map_to_assoc_rules)\n",
    "\n",
    "# Output as dataframe\n",
    "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instacart dataset\n",
    "\n",
    "With your MBA algorithm ready to be used, now it is time to work on the real dataset. For this part of the TP, download the [instacart](https://www.instacart.com/datasets/grocery-shopping-2017) dataset and read its [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) to understand how the dataset is structured. \n",
    "\n",
    "Before applying the developed algorithm on the instacart dataset, you must first filter the transactions to be in the same format defined by your algorithm (one transaction per row). To manipulate the data, we can use Spark's data frame and the SQL module presented in Section 1.\n",
    "\n",
    "The following code cell uses the Spark SQL module to read the orders from the ``order_products__train.csv`` and the detailed information from ``orders.csv`` and ``products.csv`` to construct a data frame that contains a list of all products ever purchased by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_products__train.csv\n",
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|       1|     49302|                1|        1|\n",
      "|       1|     11109|                2|        1|\n",
      "|       1|     10246|                3|        0|\n",
      "|       1|     49683|                4|        0|\n",
      "|       1|     43633|                5|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orders.csv\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2539329|      1|   prior|           1|        2|                8|                  null|\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "products.csv\n",
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "|         3|Robust Golden Uns...|      94|            7|\n",
      "|         4|Smart Ones Classi...|      38|            1|\n",
      "|         5|Green Chile Anyti...|       5|           13|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_order_prod = spark.read.csv('instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
    "print('order_products__train.csv')\n",
    "df_order_prod.show(5)\n",
    "\n",
    "df_orders = spark.read.csv('instacart/orders.csv', header=True, sep=',', inferSchema=True)\n",
    "print('orders.csv')\n",
    "df_orders.show(5)\n",
    "\n",
    "df_products = spark.read.csv('instacart/products.csv', header=True, sep=',', inferSchema=True)\n",
    "print('products.csv')\n",
    "df_products.show(5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "List of products ever purchased by each user\n",
    "\"\"\"\n",
    "# USING SQL\n",
    "df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n",
    "df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n",
    "df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n",
    "# spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n",
    "#                ' FROM orders o '\n",
    "#                ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n",
    "#                ' INNER JOIN products p    ON op.product_id = p.product_id'\n",
    "#                ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n",
    "\n",
    "\n",
    "# USING DATAFRAME OPERATIONS\n",
    "# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n",
    "# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n",
    "# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n",
    "# .orderBy(df_orders.user_id).show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Business Insights (25 points) \n",
    "\n",
    "Now, you are the data scientist. Considering only the orders from ``order_products__train.csv``, use of Spark SQL module, performing with SQL or data frame, to answer the following questions:\n",
    "\n",
    "1. What are the top 10 products which have the highest probability of being reordered (consider only products purchased at least 40 times)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+\n",
      "|product_id|        product_name|  rate|\n",
      "+----------+--------------------+------+\n",
      "|      1729|2% Lactose Free Milk|0.9348|\n",
      "|     20940|Organic Low Fat Milk|0.9130|\n",
      "|     12193|100% Florida Oran...|0.8983|\n",
      "|     21038|Organic Spelt Tor...|0.8889|\n",
      "|     31764|Original Sparklin...|0.8889|\n",
      "|     24852|              Banana|0.8842|\n",
      "|       117|  Petit Suisse Fruit|0.8833|\n",
      "|     39180|Organic Lowfat 1%...|0.8820|\n",
      "|     12384|Organic Lactose F...|0.8810|\n",
      "|     24024|      1% Lowfat Milk|0.8785|\n",
      "+----------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INNER JOIN with the product name takes too long if we run it on all order_products.\n",
    "# We select the top 10 most likely to be reordered order_products and then apply INNER JOIN.\n",
    "\n",
    "df_reordered_product_ids = spark.sql(\n",
    "    ' SELECT op.product_id, SUM(op.reordered)/COUNT(op.order_id) as rate, COUNT(op.order_id) as nb_orders, SUM(op.reordered) as nb_reordered ' \n",
    "    ' FROM order_prod op '\n",
    "    ' GROUP BY op.product_id '\n",
    "    ' HAVING nb_orders >= 40'\n",
    "    ' ORDER BY rate DESC '\n",
    "    ' LIMIT 10 ')\n",
    "df_reordered_product_ids.createOrReplaceTempView(\"reordered_product_ids\")\n",
    "\n",
    "spark.sql(\n",
    "    ' SELECT p.product_id, p.product_name, CAST(rpi.rate as decimal(5,4)) '\n",
    "    ' FROM products p '\n",
    "    ' INNER JOIN reordered_product_ids as rpi ON rpi.product_id = p.product_id'\n",
    "    ' ORDER BY rpi.rate DESC ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the busiest shopping hours for each department?\n",
    "    - Hint: plot a figure containing a time series for each department \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_departments = spark.read.csv('instacart/departments.csv', header=True, sep=',', inferSchema=True)\n",
    "\n",
    "def get_all_products_department(dep_id):\n",
    "    return spark.sql(\n",
    "         ' SELECT p.product_id '\n",
    "         ' FROM products p '\n",
    "        f' WHERE p.department_id = {dep_id}')\n",
    "\n",
    "def get_all_orders_with_products(df_prod_ids):\n",
    "    product_ids = df_prod_ids.toPandas()['product_id'].tolist()\n",
    "    return spark.sql(\n",
    "         ' SELECT DISTINCT op.order_id '\n",
    "         ' FROM order_prod op '\n",
    "         f' WHERE op.product_id IN {tuple(product_ids)} ')\n",
    "\n",
    "def get_hour_of_day_freq(df_order_ids):\n",
    "    order_ids = df_order_ids.toPandas()['order_id'].tolist()\n",
    "    return spark.sql(\n",
    "         ' SELECT o.order_hour_of_day, COUNT(o.order_id) as frequency '\n",
    "        f' FROM (SELECT * FROM orders as o WHERE o.order_id IN {tuple(order_ids)}) o '\n",
    "         ' GROUP BY o.order_hour_of_day '\n",
    "         ' ORDER BY o.order_hour_of_day')\n",
    "\n",
    "for department in df_departments.rdd.collect():\n",
    "    df_product_ids = get_all_products_department(department['department_id'])\n",
    "    df_order_ids = get_all_orders_with_products(df_product_ids)    \n",
    "    df_hour_of_day_freq = get_hour_of_day_freq(df_order_ids)\n",
    "\n",
    "    freq = df_hour_of_day_freq.toPandas()\n",
    "    freq.plot.bar(x='order_hour_of_day', y='frequency', title=f'Number of orders which included an item bought in the {department[\"department\"]} department according to the time of the day')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the size of the orders (basket size)? \n",
    "    - Hint: plot the basket size distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_items_bought = spark.sql(\n",
    "    ' SELECT COUNT(op.product_id) as nb_product'\n",
    "    ' FROM order_prod op '\n",
    "    ' GROUP BY op.order_id'\n",
    ")\n",
    "\n",
    "nb_items_bought.toPandas().hist(bins=40)\n",
    "plt.title(\"Basket size distribution\")\n",
    "plt.xlabel(\"Basket size\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run MBA for the training set (15 points)\n",
    "\n",
    "Using the orders from the ``order_products__train.csv``, create a data frame where each row contain just one column, the transaction, with the list of purchased products. For this task, you do not have to query for the product names to run the MBA algorithm. You can assume that the product name is its id. This can accelerate the algorithm since the product names can be quite big.\n",
    "\n",
    "You must report the time spent to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions\n",
    "\"\"\"\n",
    "rdd_transactions = df_order_prod.select('order_id','product_id').rdd\\\n",
    "    .reduceByKey(reduce_tuples_to_list_by_key)\\\n",
    "    .map(ensure_all_values_are_lists)\\\n",
    "    .map(lambda x: (x[0], \";\".join([str(x) for x in sorted(x[1])])))\n",
    "    \n",
    "\n",
    "df_transactions = rdd_transactions.map(format_tuples).toDF(['order_id', 'transaction']).drop('order_id')\n",
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and show the first 5 association rules\n",
    "\"\"\"\n",
    "transactions_rdd = df_transactions.rdd\n",
    "\n",
    "\n",
    "\n",
    "mba_rdd = transactions_rdd\\\n",
    "    .flatMap(map_to_patterns)\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "\n",
    "\n",
    "for x in mba_rdd.take(5):\n",
    "    for y in x:\n",
    "        print(x)\n",
    "\n",
    "#     .flatMap(map_to_subpatterns)\\\n",
    "#     .reduceByKey(reduce_tuples_to_list_by_key)\\\n",
    "#     .map(ensure_all_values_are_lists)\\\n",
    "#     .map(map_to_assoc_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 1 times, most recent failure: Lost task 0.0 in stage 75.0 (TID 1509, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1869, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 281, in mergeCombiners\n    self._spill()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 300, in _spill\n    os.makedirs(path)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 28] No space left on device: '/private/var/folders/n3/r1dt9hq50v5b9x3rj6rm3nlc0000gn/T/blockmgr-4ba37eff-a6f6-4cf4-a7c4-cac5e6fabe69/python/14081'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1869, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 281, in mergeCombiners\n    self._spill()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 300, in _spill\n    os.makedirs(path)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 28] No space left on device: '/private/var/folders/n3/r1dt9hq50v5b9x3rj6rm3nlc0000gn/T/blockmgr-4ba37eff-a6f6-4cf4-a7c4-cac5e6fabe69/python/14081'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 1 times, most recent failure: Lost task 0.0 in stage 75.0 (TID 1509, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1869, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 281, in mergeCombiners\n    self._spill()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 300, in _spill\n    os.makedirs(path)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 28] No space left on device: '/private/var/folders/n3/r1dt9hq50v5b9x3rj6rm3nlc0000gn/T/blockmgr-4ba37eff-a6f6-4cf4-a7c4-cac5e6fabe69/python/14081'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1869, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 281, in mergeCombiners\n    self._spill()\n  File \"/Users/claudiaonorato/Documents/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 300, in _spill\n    os.makedirs(path)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/Users/claudiaonorato/miniconda3/envs/py3/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 28] No space left on device: '/private/var/folders/n3/r1dt9hq50v5b9x3rj6rm3nlc0000gn/T/blockmgr-4ba37eff-a6f6-4cf4-a7c4-cac5e6fabe69/python/14081'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mba_rdd.cache()\n",
    "mba_rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run MBA for the whole dataset (15 points)\n",
    "\n",
    "As you probably noticed, even for a not so large data set (the training file has only 131209 orders), the MBA algorithm is computationally expensive. For that reason, this time, we will repeat the process, but now using the Google Cloud Platform (GCP) to create a large computer cluster. All the instructions for creating a computing cluster with spark and how to submit a job will be explained in both sessions of the laboratory. In any case, you should read the instructions given in the ``Instruction_GCP.pdf``.\n",
    "\n",
    "This time, we will work with the ``order_products__prior.csv`` file, which contains more than 3M orders.\n",
    "\n",
    "**EXPECTED OUTPUT**\n",
    "\n",
    "After you ran the MBA for the larger collection of orders, randomly select 5 products purchased in ``order_products__prior`` and print the association rules associated to those products when the product is alone in the basket. The output should be as described below:\n",
    "\n",
    "**This time,  the products' name should be printed, not its ID. You can still perform the MBA using the IDs, but this last print must show the names.  \n",
    "The output should be formatted in a table, each row containing the information of the association rules for one of the 10 products.**\n",
    "       \n",
    "Report the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and print the requested output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
